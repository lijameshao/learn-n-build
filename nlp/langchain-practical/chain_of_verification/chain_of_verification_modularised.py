# -*- coding: utf-8 -*-

"""Chain for question-answering with self-verification."""
from __future__ import annotations

import warnings
from typing import Any, Dict, List, Optional

from langchain.callbacks.manager import CallbackManagerForChainRun
from langchain.chains.base import Chain
from langchain.chains.llm import LLMChain
from langchain.chains.sequential import SequentialChain
from langchain.prompts import PromptTemplate
from langchain.pydantic_v1 import BaseModel, Field, Extra
from langchain.schema import StrOutputParser
from langchain.schema.language_model import BaseLanguageModel
from langchain.output_parsers import PydanticOutputParser

from langchain.prompts.prompt import PromptTemplate

BASELINE_RESPONSE_TEMPLATE = """{query}\n\n"""
BASELINE_RESPONSE_PROMPT = PromptTemplate(
    input_variables=["query"], template=BASELINE_RESPONSE_TEMPLATE
)

PLAN_VERIFICATIONS_FORMAT_INSTRUCTIONS = """The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"query": {"title": "Query", "description": "The user\'s query", "type": "string"}, "baseline_response": {"title": "Baseline Response", "description": "The response to the user\'s query", "type": "string"}, "facts_and_verification_questions": {"title": "Facts And Verification Questions", "description": "Facts (as the dictionary keys) extracted from the response and verification questions related to the query (as the dictionary values)", "type": "object", "additionalProperties": {"type": "string"}}}, "required": ["query", "baseline_response", "facts_and_verification_questions"]}
```"""

PLAN_VERIFICATIONS_TEMPLATE = """
Given the below Query and Answer, generate a series of verification questions that test the factual claims in the original baseline response.
For example if part of a longform model response contains the statement “The Mexican–American War
was an armed conflict between the United States and Mexico from 1846 to 1848”, then one possible
verification question to check those dates could be “When did the Mexican American war start and end?”

Query: {query}
Answer: {baseline_response}

<fact in passage>, <verification question, generated by combining the query and the fact>

{format_instructions}
"""

PLAN_VERIFICATIONS_PROMPT = PromptTemplate(
    input_variables=["query", "baseline_response"],
    template=PLAN_VERIFICATIONS_TEMPLATE,
    partial_variables={"format_instructions": PLAN_VERIFICATIONS_FORMAT_INSTRUCTIONS},
)

EXECUTE_VERIFICATIONS_TEMPLATE = """{verification_question}\n\n"""
EXECUTE_VERIFICATIONS_PROMPT = PromptTemplate(
    input_variables=["verification_question"], template=EXECUTE_VERIFICATIONS_TEMPLATE
)

REVISED_RESPONSE_TEMPLATE = """Given the ORIGINAL_QUESTION and the ORIGINAL_RESPONSE,
revise the ORIGINAL_RESPONSE (if applicable) such that it is consistent with information in VERIFIED_SOURCE.
Only keep consistent information.

<ORIGINAL_QUESTION>
{query}

<ORIGINAL_RESPONSE>
{baseline_response}

<VERIFIED_SOURCE>
{verified_responses}

Final response:
"""
REVISED_RESPONSE_PROMPT = PromptTemplate(
    input_variables=["query", "baseline_response", "verified_responses"],
    template=REVISED_RESPONSE_TEMPLATE,
)


class PlanVerificationsOutputModel(BaseModel):
    query: str = Field(description="The user's query")
    base_response: str = Field(description="The response to the user's query")
    facts_and_verification_questions: dict[str, str] = Field(
        description="Facts (as the dictionary keys) extracted from the response and verification questions related to the query (as the dictionary values)"
    )


class CoVeChain(Chain):
    """Chain of Verification (CoVe)

    Example:
        .. code-block:: python

            from langchain.llms import OpenAI
            from langchain_experimental.chain_of_verification import CoVeChain
            llm = OpenAI(temperature=0)
            cove_chain = CoVeChain.from_llm(llm)
    """

    cove_chain: SequentialChain

    llm: Optional[BaseLanguageModel] = None
    baseline_response_prompt: PromptTemplate = BASELINE_RESPONSE_PROMPT
    plan_verifications_prompt: PromptTemplate = PLAN_VERIFICATIONS_PROMPT
    plan_verifications_output_model: BaseModel = PlanVerificationsOutputModel
    execute_verifications_prompt: PromptTemplate = EXECUTE_VERIFICATIONS_PROMPT
    revised_response_prompt: PromptTemplate = REVISED_RESPONSE_PROMPT
    input_key: str = "query"  #: :meta private:
    output_key: str = "result"  #: :meta private:

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid
        arbitrary_types_allowed = True

    @property
    def input_keys(self) -> List[str]:
        """Return the singular input key.

        :meta private:
        """
        return [self.input_key]

    @property
    def output_keys(self) -> List[str]:
        """Return the singular output key.

        :meta private:
        """
        return [self.output_key]

    def _call(
        self,
        inputs: Dict[str, Any],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> Dict[str, str]:
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        query = inputs[self.input_key]

        output = self.cove_chain({"query": query}, callbacks=_run_manager.get_child())
        return {self.output_key: output["result"]}

    @property
    def _chain_type(self) -> str:
        return "cove_chain"

    @classmethod
    def from_llm(
        cls,
        llm: BaseLanguageModel,
        baseline_response_prompt: PromptTemplate = BASELINE_RESPONSE_PROMPT,
        plan_verifications_prompt: PromptTemplate = PLAN_VERIFICATIONS_PROMPT,
        execute_verifications_prompt: PromptTemplate = EXECUTE_VERIFICATIONS_PROMPT,
        revised_response_prompt: PromptTemplate = REVISED_RESPONSE_PROMPT,
        **kwargs: Any,
    ) -> CoVeChain:
        plan_verifications_output_parser = PydanticOutputParser(
            pydantic_object=PlanVerificationsOutputModel
        )

        baseline_response_chain = baseline_response_prompt | llm | StrOutputParser()
        plan_verifications_chain: PlanVerificationsOutputModel = (
            {"baseline_response": baseline_response_chain}
            | plan_verifications_prompt
            | llm
            | plan_verifications_output_parser
        )
        verification_questions = list(
            plan_verifications_chain.facts_and_verification_questions.values()
        )
        execute_verification_chain = (
            execute_verifications_prompt | llm | StrOutputParser()
        )

        verify_results_str = ""
        for i in range(len(verification_questions)):
            question = verification_questions[i]
            answer = execute_verification_chain.invoke(question)
            answer = answer.lstrip("\n")
            verify_results_str += f"Question: {question}\nAnswer: {answer}\n\n"

        # cove_chain = (
        #     {"result": synopsis_prompt | llm | StrOutputParser()}
        #     | review_prompt
        #     | llm
        #     | StrOutputParser()
        # )
        return cls(
            cove_chain=cove_chain,
            **kwargs,
        )
